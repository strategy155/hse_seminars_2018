{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam\n",
    "\n",
    "    \n",
    "1. Bloom filter\n",
    "    1. Universal hash families: properties\n",
    "    1. Definition aka data structure\n",
    "    1. Estimation of false positive rate (with derivation)\n",
    "    \n",
    "1. Numerical optimization\n",
    "    1. Problem statement\n",
    "    1. Gradient Decent, Pros and Cons\n",
    "    1. Stochastic Gradient Decent, Pros and Cons\n",
    "    1. Necessary conditions for Loss Function to apply SGD\n",
    "    1. Variations of SGD: SGD with momentum, RmsProp, Adam\n",
    "    \n",
    "1. Model Complexity\n",
    "    1. Notion of model complexity\n",
    "    1. Notion of overfitting and regularization\n",
    "    1. Explaining overfitting and regularization through generalization bounds\n",
    "    1. No Free Lunch Theorem: interpretation\n",
    "    \n",
    "1. Model Perfomance\n",
    "    1. Bias variance decomposition (without derivation)\n",
    "    1. Learning curves: behavior\n",
    "    1. Validation curves: behavior\n",
    "    \n",
    "1. Validation\n",
    "    1. Common Validation pipeline\n",
    "    1. Leave-one-out, Pros and Cons\n",
    "    1. Cross-validation, Pros and Cons\n",
    "    1. hold-out, Pros and Cons\n",
    "    \n",
    "1. Regularization in linear models\n",
    "    1. 3 interpretations of L2 regularization (without derivation)\n",
    "    1. L1 regularization: properties (without derivation).\n",
    "    1. ElasticNet regularization: properties.\n",
    "    \n",
    "1. Logistic Regression\n",
    "    1. Problem statement\n",
    "    1. Loss function\n",
    "    1. Multiclass classification\n",
    "    1. Properties of the solution\n",
    "    \n",
    "1. Support Vector Machines\n",
    "    1. Notion of margin\n",
    "    1. Problem statement\n",
    "    1. Loss function\n",
    "    1. Properties of the solution\n",
    "    1. Kernel Trick: RKHS and kernels\n",
    "    \n",
    "1. Decision Trees\n",
    "    1. General construction algorithm\n",
    "    1. Properties of the solution\n",
    "    1. Splitting criteria\n",
    "    \n",
    "1. Ensembles over decision trees\n",
    "    1. Notion of bagging and boosting\n",
    "    1. Connection to Bias variance decomposition\n",
    "    1. out-of-bag score\n",
    "    1. AdaBoost vs Gradient Boosting\n",
    "    \n",
    "1. Feed Forward NN\n",
    "    1. Net Architecture\n",
    "    1. Neuron Architecture\n",
    "    1. FFNN as universal approximators\n",
    "    1. Computational Graphs and Backpropagation\n",
    "    1. Dropout: properties\n",
    "    \n",
    "1. Linear Dimension Reduction, part 1\n",
    "    1. Problem statement\n",
    "    1. Random Projections\n",
    "    1. NMF\n",
    "    \n",
    "1. Linear Dimension Reduction, part 2\n",
    "    1. PCA\n",
    "    1. Truncated SVD\n",
    "    \n",
    "1. Nonlinear Dimension Reduction\n",
    "    1. TSNE\n",
    "    1. AutoEncoders: Loss Function and its variants.\n",
    "    \n",
    "1. Recurrent Neural Networks\n",
    "    1. General Architecture\n",
    "    1. Backpropagation Throug Time\n",
    "    1. Vanishing and exploding Gradients\n",
    "    1. LSTM and GRU\n",
    "    \n",
    "1. Embeddings\n",
    "    1. Distributional hypothesis\n",
    "    1. Word2Vec: skipgram and CBOW model\n",
    "    1. Glove\n",
    "    1. FastText and other subword embeddings\n",
    "    1. Solutions for extreme multiclass classification\n",
    "    1. Context-aware embeddings\n",
    "   \n",
    "1. Metric Learning\n",
    "    1. Propertie of KNN\n",
    "    1. Siamese metworks\n",
    "    1. Parametrized distance functions and learning embeddings\n",
    "    1. Triplet Loss\n",
    "    1. LSH\n",
    "    \n",
    "1. Topic Modeling\n",
    "    1. Problem statement\n",
    "    1. LSA\n",
    "    1. PLSA\n",
    "    1. LDA\n",
    "    1. topic coherence\n",
    "    \n",
    "1. Hidden Markov Models\n",
    "    1. Models with latent variables\n",
    "    1. Definion of HMM\n",
    "    1. Viterbi algorithm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
