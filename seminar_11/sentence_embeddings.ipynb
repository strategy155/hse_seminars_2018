{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Embeddings\n",
    "\n",
    "### Outline:\n",
    "\n",
    "1. Evalutaion\n",
    "1. Unsupervised\n",
    "1. Strong Baselines\n",
    "1. Universal Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "SentEval from Facebook Research.   \n",
    "SentEval is a framework of evaluation of sentence embeddings on a number of transfer learning tasks.  \n",
    "\n",
    "<img src=\"images/sent_eval_results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1 Unsupervised Embeddings\n",
    "\n",
    "## 1.1 Skip-Thought\n",
    "\n",
    "An extension of skip-gram model: given current sentence predict adjacent sentences. \n",
    "\n",
    "Problems: too heavy decoder.  \n",
    "\n",
    "<img src=\"images/skip_thought.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 . Quick Thought\n",
    "\n",
    "Optimized version of Skip-Thoughts Model:  let's get rid of decoder and predict next sentence label against random alternatives.\n",
    "\n",
    "<img src=\"images/quick_thought.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 Infersent\n",
    "\n",
    "Sentence encoder: bidirectional LSTM with max polling.\n",
    "<img src=\"images/infersent_1.png\"  style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Trained on Stanfolrd Natural Language Inference (SNLI) dataset.\n",
    "Given 2 sentences (premise and hypothesis), make prediction if:\n",
    "1. the 2nd sentence is an entailment of the 1st\n",
    "2. the 2nd sentence is a contradiction of the 1st\n",
    "3. sentences are neutral \n",
    "\n",
    "<img src=\"images/infersent_2.png\"  style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2 Strong Baselines\n",
    "\n",
    "## 2.1 SIF\n",
    "\n",
    "### In Short\n",
    "\n",
    "1. Compute sentence embedding as a weighted sum of word embeddings\n",
    "1. Form a matrix of sentence embeddings X\n",
    "1. Substract form sentenc embedding the projection onto the first eigenvector X.\n",
    "\n",
    "### Theory behind\n",
    "\n",
    "Idea:  \n",
    "Then treat corpus generation as a random walk of a discourse vector $c_t$. Then, probability of emmiting word $w$ at time t  is \n",
    "$$ P( w | c_s) \\propto {\\exp(<c_t, v_w>)}$$, where $v_w$ is embedding of word $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose discource vector does not change much inside a sentence. Then, given sentence $s$, introduce a discourse vector $c_s \\in R^d$  \n",
    "\n",
    "Let $p(w)$ - unigram probability  \n",
    "Let $\\alpha, \\beta \\in R$  \n",
    "Let $c_0$ is a common discourse vector, correction term for the most frequent discourse that is often related to syntax.  \n",
    "Let $\\hat c_s = \\beta c_s  + (1 - \\beta)c_0 $ - smoothed discourse vector\n",
    "\n",
    "\n",
    "Probability of emmiting word $w$ in a sentence s:\n",
    "$$ P( w | c_s) = \\alpha p(w) + (1 - \\alpha) \\frac {\\exp(<\\hat c_w, v_w>)} {Z_{\\hat c_s}}$$.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. $v_s \\leftarrow  \\frac 1 {|s|} \\sum_{i=1}^{|s|} \\frac {\\alpha}  {\\alpha + p(w)} v_w $\n",
    "1. form matrix $X$ whose columns are ${v_s : s \\in S}$, \n",
    "1. $v_s \\leftarrow v_s - u u^T v_s$, where $u$ - the first singular vector of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Random Methods\n",
    "\n",
    "1. **Bag of random embedding projections**\n",
    "$$h  = f_pool ( U e_i )$$,\n",
    "where $h \\in R^D$ - sentence embedding,    \n",
    "$e_i \\in R^d$ - word embedding,  \n",
    "$U \\in R^{Dxd}$ - random matrix  \n",
    "\n",
    "\n",
    "1. **Random LSTM**\n",
    "$$ h  = f_pool ( LSTM(e_1, ..., e_n) )$$,\n",
    "where $v_s \\in R^D$ - sentence embedding,   \n",
    "$e_i \\in R^d$ - word embedding,  \n",
    "$LSTM$ - randomly initialized LSTM  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3 Universal Sentence Embeddings\n",
    "\n",
    "Sentence embeddings can profit greatly from multitask learning.\n",
    "<img src=\"images/multitask.png\"  style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Universal Sentence Encoder from Google has 2 variants of encoder:\n",
    "1. Transformer encoder  \n",
    "2. Deep Averaging Network (DAN) = average of word embeddings + MLP\n",
    "\n",
    "\n",
    "<img src=\"images/google.png\"  style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Papers\n",
    "\n",
    "1. [SIF] A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS, ICLR 2017\n",
    "1. [GenSen] Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning, ICLR 2018\n",
    "1. [Google] \n",
    "1. NO TRAINING REQUIRED: EXPLORING RANDOM ENCODERS FOR SENTENCE CLASSIFICATION, ILCR 2019\n",
    "1. Universal Sentence Encoder\n",
    "1. [Skip Thought]Skip-Thought Vectors, Kiros et al. 2015\n",
    "1. [Quick Thought] An efficient framework for learning sentence representations, ICLR 2018\n",
    "1. [InferSent] Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, EMNLP 2017"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
