{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq\n",
    "\n",
    "\n",
    "Outline:  \n",
    "1. General Architecture\n",
    "1. Teacher Forcing\n",
    "1. Beam Search\n",
    "1. Attention\n",
    "1. Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 General Architecture\n",
    "\n",
    "<img src=\"images/seq2seq.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "From source sequence generate target sequence.  \n",
    "\n",
    "\n",
    "**Encoder**:   \n",
    "$h_t = LSTM(h_{t-1}, x_t)$ - encoder hidden state     \n",
    "$e_t = out_e(h_t)$ - output of encoder at time $t$     \n",
    "\n",
    "**Decoder**:  \n",
    "$s_t = LSTM(s_{t-1}, y_{t-1})$ - decoder hidden state  \n",
    "$g_t = out_g(s_t)$ - output of decoder at time $t$  \n",
    "$p_t = softmax(g_t)$ - probabilities of tokens at time $t$  \n",
    "$y_t = argmax(p_t)$ - predicted token at time $t$  \n",
    "$s_o = h_T$ - initial decoder hidden state is the last encoder hidden state.  \n",
    "\n",
    "\n",
    "Seq2seq is a classification task. Because at every time step you have to choose what token to output.   \n",
    "Loss function is very similar to autoregression case = is an average of cross-entropy loss on tokens. \n",
    "Since it is an average, it doesn't depend on the sequence length. \n",
    "\n",
    "$$ Loss(S_{pred}, S_{target}) = \\frac 1 {|S_{target}|} \\sum_{i=1}^{|S_{target}|} cross\\_entropy(y_i, \\hat y_i)$$\n",
    "where  \n",
    "$S_{pred}, S_{target}$ - predicted and target sequences.  \n",
    "$y_i, \\hat y_i$ - target token and predicted token for corresponding sequences.  \n",
    "If predicted sequence is shorter than the target sequence, it should be padded to target sequence length.  \n",
    "If predicted sequence is longer than the target sequence, it should be cutted to target sequence length.    \n",
    "Because you train your model with mini-batches, you have to pad your target sequences to have common length.  \n",
    "Padding value should not be counted as an error.  \n",
    "All that is done for your by `torch.nn.CrossEntropyLoss(ignore_index = <your padding value>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Teacher forcing \n",
    "\n",
    "There is a problem with training the decoder.  \n",
    "Because at $y_{t}$ depends on $y_{t-1}, ..., y_0$, if at some timestamp a wrong token is predicted, the rest of the sequence will be wrong too.   \n",
    "So teacher forcing method was introduced.  \n",
    "\n",
    "<img src=\"images/teacher.png\" style=\"height:400px\">\n",
    "\n",
    "At the training phase, at every timestep you give the decoder **true previous token $x_{t-1}$** to predict the current one $y_t$.  \n",
    "At the inference phase, at every timestep you give the decoder **predicted previous token $y_{t-1}$** to predict the current one $y_t$.\n",
    "\n",
    "Well, there is another problem with teacher forcing: the model is tought to do something different from it actually should do. Conditioning on the true previous token is a bit easier.  \n",
    "Though, in pytorch you can mix both regimes: some batches train with teacher forcing and others - with vanilla BPTT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Beam search \n",
    "\n",
    "<img src=\"images/beam.png\" style=\"height:500px\">\n",
    "\n",
    "There is another problem with seq2seq models: prediction of discrete tokens at the inference phase.  \n",
    "In general, you want to generate the most probable sequence given the input. And now you that by predicting the most probable token at each timestamp, which is actually wrong.  \n",
    "Because such greedy search is not graunted to give you the optimal solution.  \n",
    "\n",
    "To overcome this problem, beam search was introduced. \n",
    "Basically it says: at every timestamp lets evaluate probabilities of already generated sequences + appended new predicted token. And then keep track only of top-k most probable ones.  \n",
    "This gives you better, yet still not optimal solution.  \n",
    "\n",
    "1. at time t you have top-k already generated subsequences ${(y_0^(1), .., y_{t-1}^(1)), (y_0^(k), .., y_{t-1}^(k))}$\n",
    "1. insert them into generator and get top-k most probable tokens for each subsequence (total $k^2$ variants)\n",
    "1. evaluate probabilities of all new possible subsequences of length $t+1$. \n",
    "1. select top-k subsequences ${(y_0^(1), .., y_{t-1}^(1)), (y_0^(k), .., y_{t-1}^(k))}$\n",
    "1. after max_len is reached or terminal symbol is generated, select most probable sequence as output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Attention\n",
    "\n",
    "<img src=\"images/attention.jpg\" style=\"height:500px\">\n",
    "\n",
    "Another problem with vanilla seq2seq: for long sequences it's hard to put all information about sorce sequence into the intermidiate encoder state. \n",
    "\n",
    "Attention is a mechanism of conditioning of every output on a weighted sum of source inputs.\n",
    "\n",
    "Introduce attention through new function $f$:  \n",
    "$ \\alpha_{t'} = f(g_{t-1}, e_{t'}) $ - weights of source tokens.    \n",
    "$ \\bar \\alpha = softmax(\\alpha) $ - normalize weights.  \n",
    "$ c_t  = \\sum_{t'=0}^T \\bar \\alpha_{t'} e_{t'}$ - weighted sum\n",
    "\n",
    "**Encoder**:   \n",
    "$h_t = LSTM(h_{t-1}, x_t)$ - encoder hidden state     \n",
    "$e_t = out_e(h_t)$ - output of encoder at time $t$     \n",
    "\n",
    "**Decoder**:  \n",
    "$s_t = LSTM(s_{t-1}, [y_{t-1}, c_t])$ - decoder hidden state  \n",
    "$g_t = out_g(s_t)$ - output of decoder at time $t$  \n",
    "$p_t = softmax(g_t)$ - probabilities of tokens at time $t$  \n",
    "$y_t = argmax(p_t)$ - predicted token at time $t$  \n",
    "$s_o = h_T$ - initial decoder hidden state is the last encoder hidden state.  \n",
    "\n",
    "\n",
    "Usual choices for attention functions:\n",
    "\n",
    "$f(h, e) = h^T e$ - dot  \n",
    "$f(h, e) = h^T W e$ - general    \n",
    "$f(h, e) = v^T tanh(W [h, e])$  concat  \n",
    "\n",
    "\n",
    "Basic attention mechanisms:  \n",
    "\n",
    "<img src=\"images/attn2.png\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "\n",
    "Bonus: interpretable models - because every predicted token is conditioned on a weighted sum of input tokens, it means, that you can see wich tokens were most infuentional for the prediced one.    \n",
    "\n",
    "\n",
    "<img src=\"images/viz.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Neural Machine Translation\n",
    "\n",
    "NMT usually evaluated with BLEU and ROUGE scores.  \n",
    "For every case several references can be provided.  \n",
    "\n",
    "### BLEU score\n",
    "\n",
    "<img src=\"images/bleu.jpg\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "### ROUGE score\n",
    "\n",
    "Overlap between n-grams of hypothesis and reference sentences\n",
    "\n",
    "$$ROUGE-N = \\frac {number\\_of\\_common\\_Ngrams} {total\\_number\\_of\\_Ngrams\\_in\\_reference}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
