{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about embeddings\n",
    "\n",
    "Outline:  \n",
    "1. Char CNN embedding\n",
    "1. FastText\n",
    "1. BPE\n",
    "1. Elmo\n",
    "1. Siamise Networks\n",
    "1. Triplet Loss\n",
    "1. Hard Negative Mining\n",
    "1. KNN\n",
    "1. LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some routines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.label)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator, criterion)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Char CNN Embeddings\n",
    "\n",
    "Treat token as a sequence of symbols.  \n",
    "Stack 1-dim Conv layer + polling layer to produce fixed size embedding of the token.  \n",
    "\n",
    "Use `torchtext.data.NestedField` for char embeddings\n",
    "\n",
    "<img src=images/char.png height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_field = data.Field(tokenize=list, unk_token=\"<cunk>\", pad_token=\"<cpad>\",\n",
    "                                   init_token=\"<w>\", eos_token=\"</w>\")\n",
    "CHARS = data.NestedField(nesting_field, init_token=\"<s>\", eos_token=\"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "classes={\n",
    "    'negative':0,\n",
    "    'neutral':1,\n",
    "    'positive':2\n",
    "}\n",
    "\n",
    "TEXT = data.Field(tokenize=list, \n",
    "                  unk_token=\"<cunk>\", \n",
    "                  pad_token=\"<cpad>\",\n",
    "                  init_token=\"<w>\", \n",
    "                  eos_token=\"</w>\")\n",
    "\n",
    "CHAR = data.NestedField(\n",
    "            TEXT,\n",
    "            init_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            fix_length=30, # max token length\n",
    "            dtype=tt.long,\n",
    "            include_lengths=True,\n",
    "        )\n",
    "\n",
    "\n",
    "LABEL = data.LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
    "\n",
    "dataset = data.TabularDataset('../seminar_10/Tweets.csv', format='csv', \n",
    "                         fields=[(None, None),('label', LABEL), (None, None),(None, None),\n",
    "                                 ('char', CHAR)], \n",
    "                         skip_header=True)\n",
    "\n",
    "CHAR.build_vocab(dataset, min_freq=5)\n",
    "LABEL.build_vocab(dataset)\n",
    "\n",
    "train, valid = dataset.split(0.7, stratified=True)\n",
    "\n",
    "print(len(CHAR.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<cunk>', '<cpad>', '<s>', '</s>', '<w>', '</w>', 'a', 'b']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@', 'V', 'i', 'r', 'g', 'i', 'n', 'A', 'm', 'e', 'r', 'i', 'c', 'a'],\n",
       " ['W', 'h', 'a', 't'],\n",
       " ['@', 'd', 'h', 'e', 'p', 'b', 'u', 'r', 'n'],\n",
       " ['s', 'a', 'i', 'd', '.']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.examples[0].char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[['<w>',\n",
       "    '<s>',\n",
       "    '</w>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<w>',\n",
       "    '@',\n",
       "    'V',\n",
       "    'i',\n",
       "    'r',\n",
       "    'g',\n",
       "    'i',\n",
       "    'n',\n",
       "    'A',\n",
       "    'm',\n",
       "    'e',\n",
       "    'r',\n",
       "    'i',\n",
       "    'c',\n",
       "    'a',\n",
       "    '</w>'],\n",
       "   ['<w>',\n",
       "    'W',\n",
       "    'h',\n",
       "    'a',\n",
       "    't',\n",
       "    '</w>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<w>',\n",
       "    '@',\n",
       "    'd',\n",
       "    'h',\n",
       "    'e',\n",
       "    'p',\n",
       "    'b',\n",
       "    'u',\n",
       "    'r',\n",
       "    'n',\n",
       "    '</w>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<w>',\n",
       "    's',\n",
       "    'a',\n",
       "    'i',\n",
       "    'd',\n",
       "    '.',\n",
       "    '</w>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<w>',\n",
       "    '</s>',\n",
       "    '</w>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>'],\n",
       "   ['<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>',\n",
       "    '<cpad>']]],\n",
       " [6],\n",
       " [[3,\n",
       "   16,\n",
       "   6,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR.pad([dataset.examples[0].char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6],\n",
       " [[3,\n",
       "   16,\n",
       "   6,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, seq_len, words_len = CHAR.pad([dataset.examples[0].char])\n",
    "seq_len, words_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCharEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, char_vocab_size, char_dim, embed_dim, hidden_size, filters=None):\n",
    "        super(ConvCharEmbedding, self).__init__()\n",
    "\n",
    "        self.char_embed = nn.Embedding(char_vocab_size, char_dim)\n",
    "\n",
    "        if filters is None:\n",
    "            filters = [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256]]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(char_dim, out_channels=n_channels, kernel_size=k, padding=int((k-1)/2)) for k, n_channels in filters\n",
    "        ])\n",
    "\n",
    "        input_size = np.sum(x[1] for x in filters)\n",
    "        \n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: shape (B, W, L)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.char_embed(x)\n",
    "\n",
    "        batch_size, n_words, length, embed_size = x.size()\n",
    "\n",
    "        x = x.transpose(2,3).contiguous().view(-1, embed_size, length)\n",
    "\n",
    "        out = []\n",
    "        for conv in self.convs:\n",
    "            z = conv(x)\n",
    "            z = F.relu(z)\n",
    "            z = F.max_pool1d(z, z.size(2)).squeeze()\n",
    "            out.append(z)\n",
    "\n",
    "        x = tt.cat(out, -1)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.contiguous().view(batch_size, n_words, -1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denaas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193d14b060b4870be88a36c820cef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.91809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7deba8df301a45fc8d67b7bc43424907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.74027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87470d6e548541f19c25c3793449bd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.63047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd24326f9164ba298e3a2e650d1c4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.71646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9e0e12d4784138b2b5a67e9919d710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.59166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcff738a2f3b4f3080f8b7b74963c825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.58378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77a286df87643b7ab19343b13fb845d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 6', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.56827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3f2c9b587c413ca7fde5f615427595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 7', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.56845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1b240aae104abb8d4d0971d0575ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 8', max=321, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.59877\n",
      "Early stopping! best epoch: 6 val 0.56827\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_size, char_dim, embed_size, hidden_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = ConvCharEmbedding(char_vocab_size, char_dim, embed_size, hidden_size=embed_size*2)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x, x_lengths, _ = batch.char\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = MyModel(len(CHAR.vocab.itos),\n",
    "                char_dim=25,\n",
    "                embed_size=100,\n",
    "                hidden_size=128,\n",
    "               )\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train, valid),\n",
    "    batch_sizes=(batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.char),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=10, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2 FastText\n",
    "\n",
    "Word is devided into subword n-grams with word boundaries:  \n",
    "`<where> -> <wh, whe, her, ere, re>`  \n",
    "\n",
    "Let  \n",
    "$w$ - word,  \n",
    "$c$ - context vector  \n",
    "$G_w \\in \\{ 1, ..., G \\}$ - set of n-grams appeared in word $w$  \n",
    "$z_g$ - vector embedding of n-gram $g$\n",
    "\n",
    "Assumption: represent word embedding as a sum of n-grams' embeddings  \n",
    "\n",
    "Then, scoring function:  \n",
    "$$ s(w,c) = \\sum_{g \\in G_w} z_g^T v_c $$\n",
    "\n",
    "Probability of context word:  \n",
    "$$ p(w_c | w_t) = \\frac { \\exp^{s(w_t, w_c) }} { \\sum_{j=1}^W \\exp^{s(w_t, j)} }$$\n",
    "\n",
    "Loss function:  \n",
    "\n",
    "$$ \\sum_{t=1}^T [ \\sum_{c \\in C_t} \\log (1 + \\exp^{- s(w_t, w_c)}) + \\sum_{n \\in N_{t,c}} \\log (1 + \\exp^{ s(w_t, n)}) ]  \\rightarrow \\min_{z, w}$$\n",
    "\n",
    "\n",
    "Rather good implementation of FastText you can found in gensim. `gensim.models.fasttext.FastText`  \n",
    "Interface is similar to Word2Vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Byte Pair Encoding (BPE)\n",
    "\n",
    "In NLP, BPE is a simple form of sequence compression when the most common pair of consecutive tokens (chars) is replaced with a token that does not occur within that data.\n",
    "\n",
    "`a,a,b,a,a,c,a ->  aa, b, aa, c, a`\n",
    "\n",
    "`https://github.com/bheinzerling/bpemb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁sim', 'ple', '▁res', 'ol', 'ution']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bpemb\n",
    "\n",
    "bpe_en = bpemb.BPEmb(lang='en', vs=1000)\n",
    "bpe_en.encode('simple resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 ELMO\n",
    "\n",
    "https://arxiv.org/pdf/1802.05365\n",
    "    \n",
    "ELMO is an example of context embeddings: embedding depends not only on the word, but on whole sentence.\n",
    "\n",
    "ELMO uses char CNN for token embeddings on 0 layer.  \n",
    "\n",
    "You can find pretrained models in `allennlp.modules.elmo.Elmo` and `https://github.com/HIT-SCIR/ELMoForManyLangs`  \n",
    "\n",
    "<img src=images/elmo1.png height=300/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denaas/anaconda3/lib/python3.6/site-packages/allennlp/nn/util.py:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [['First', 'sentence', '.', 'custom', 'service'], ['Another', '.']]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Siamese Networks\n",
    "\n",
    "<img src=images/siam1.jpeg height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199530</td>\n",
       "      <td>301049</td>\n",
       "      <td>301050</td>\n",
       "      <td>As a Canadian student, is it wiser to complete...</td>\n",
       "      <td>How much will it cost to Indian student to stu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387099</td>\n",
       "      <td>29541</td>\n",
       "      <td>519407</td>\n",
       "      <td>What is your favorite Indian sweet dish?</td>\n",
       "      <td>What's your favorite Indian dish? Why?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>337316</td>\n",
       "      <td>464776</td>\n",
       "      <td>464777</td>\n",
       "      <td>Is there proof of Jon being Rhaegar and Lyanna...</td>\n",
       "      <td>Where does GRRM imply that Jon Snow is Rhaegar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164415</td>\n",
       "      <td>255489</td>\n",
       "      <td>255490</td>\n",
       "      <td>Knowing how Prithviraj's last 3 films were flo...</td>\n",
       "      <td>Which is the Best Comedy scene in Malayalam ci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>382707</td>\n",
       "      <td>514592</td>\n",
       "      <td>514593</td>\n",
       "      <td>What causes damage to the somatosensory cortex...</td>\n",
       "      <td>What causes damage to the somatosensory cortex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  199530  301049  301050  As a Canadian student, is it wiser to complete...   \n",
       "1  387099   29541  519407           What is your favorite Indian sweet dish?   \n",
       "2  337316  464776  464777  Is there proof of Jon being Rhaegar and Lyanna...   \n",
       "3  164415  255489  255490  Knowing how Prithviraj's last 3 films were flo...   \n",
       "4  382707  514592  514593  What causes damage to the somatosensory cortex...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  How much will it cost to Indian student to stu...             0  \n",
       "1             What's your favorite Indian dish? Why?             0  \n",
       "2  Where does GRRM imply that Jon Snow is Rhaegar...             0  \n",
       "3  Which is the Best Comedy scene in Malayalam ci...             0  \n",
       "4  What causes damage to the somatosensory cortex...             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from Quora duplicate detection\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2019 12:48:59 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "xq1_train = batch_to_ids(df_train.question1.values)\n",
    "xq2_train = batch_to_ids(df_train.question2.values)\n",
    "y_train = tt.from_numpy(df_train.is_duplicate.values).float()\n",
    "\n",
    "xq1_val = batch_to_ids(df_val.question1.values)\n",
    "xq2_val = batch_to_ids(df_val.question2.values)\n",
    "y_val = tt.from_numpy(df_val.is_duplicate.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(xq1_train, xq2_train, y_train), batch_size=batch_size)\n",
    "val_loader = DataLoader(TensorDataset(xq1_val, xq2_val, y_val), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss = model(batch)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, elmo, criterion):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.elmo = elmo\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.fc = nn.Linear(1024*2, 128)\n",
    "        \n",
    "        self.out = nn.Linear(128*3, 1)\n",
    "        \n",
    "    def branch(self, x):\n",
    "        x = self.elmo(x)['elmo_representations']\n",
    "        x = tt.cat(x, dim=-1)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        q1, q2, y = batch\n",
    "        \n",
    "        q1 = self.branch(q1)\n",
    "        q2 = self.branch(q2)\n",
    "        \n",
    "        # simetric functions\n",
    "        x = tt.cat([tt.abs(q1-q2), q1*q2, q1+q2], dim=-1)\n",
    "        \n",
    "        x = self.out(x).squeeze(1)\n",
    "        loss = self.criterion(x,y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "model = MyModel(elmo, nn.BCEWithLogitsLoss())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nn_train(model, train_loader, val_loader, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Triplet loss\n",
    "\n",
    "Distance to samples from the same class should be less than to samples from other classes\n",
    "\n",
    "Euclidean distance: \n",
    "<img src=images/triplet.png height=200/>\n",
    "\n",
    "Cosine distance: \n",
    "<img src=images/triplet2.png height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
    "    return F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed)\n",
    "    \n",
    "    \n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        ...\n",
    "        \n",
    "    def branch(self, x):\n",
    "        ....\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        \n",
    "        anchor = self.branch(anchor)\n",
    "        pos = self.branch(pos)\n",
    "        neg = self.branch(neg)\n",
    "        \n",
    "        return triplet_loss(anchor, pos, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Hard Negative Mining\n",
    "\n",
    "\n",
    "Sometimes, if you use random samples as negative examples, classification may be too easy for you model.  \n",
    "You can consider taking samples from previous epoch, where your model made mistakes, as negative examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 K-Nearest Neighbors (KNN)\n",
    "\n",
    "Training complexity: O(1) - just remember all train set  \n",
    "Inference complexity: O(n) - have to compare each test sample with all train samples  \n",
    "\n",
    "<img src=images/knn.png height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Locale Sensitive Hashing (LSH)\n",
    "\n",
    "Good implementation can be found here `https://github.com/spotify/annoy`\n",
    "\n",
    "Definition:  \n",
    "LSH family $F$ is a family of hash functions that maps metric space $M$ to set of buckets $S$.  \n",
    "$$ h: M \\rightarrow S $$\n",
    "\n",
    "Let  \n",
    "$p,q \\in M $ - points in space  \n",
    "$d$ be the metric in $M$  \n",
    "$c$ - some scalar, $c > 1$\n",
    ", then for $h \\in F$:  \n",
    "\n",
    "* if $d(p,q) \\leq R$ then $P[h(p) = h(q)] \\geq p_1$  \n",
    "* if $d(p,q) \\geq cR$ then $P[h(p) = h(q)] \\leq p_2$  \n",
    "\n",
    "And family $F$ is called $(R, cR, p_1, p_2)$ - sensitive\n",
    "\n",
    "\n",
    "Assumption: uniform distribution\n",
    "    \n",
    "<img src=images/lsh1.jpeg height=400/>\n",
    "\n",
    "Amplification:  \n",
    "\n",
    "1. AND construction\n",
    "\n",
    "Define new family of hash functions $G = {g}$, where each consists of k hash functions from $F$ chosen at random $g = h_1, ..., h_k$.\n",
    "\n",
    "$g(p) = g(q)$ iff $h_i(p) = h_i(q)$ **for all** $i$  \n",
    "\n",
    "Then, family $G$ is $(d_1, d_2, p_1^k, p_2^k)$ - sensitive\n",
    "\n",
    "2. OR construction\n",
    "\n",
    "Define new family of hash functions $G = {g}$, where each consists of k hash functions from $F$ chosen at random $g = h_1, ..., h_k$.\n",
    "\n",
    "$g(p) = g(q)$ iff $h_i(p) = h_i(q)$ **at least for one** $i$  \n",
    "\n",
    "Then, family $G$ is $(d_1, d_2, 1 - (1 - p_1)^k, 1 - (1- p_2)^k)$ - sensitive\n",
    "\n",
    "LSH maps:  \n",
    "<img src=images/lsh2.png height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
